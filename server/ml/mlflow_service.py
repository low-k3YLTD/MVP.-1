#!/usr/bin/env python3\nimport mlflow\nimport mlflow.sklearn\nimport json\nimport sys\nfrom typing import Dict, Any, Optional\nfrom datetime import datetime\n\nclass MLflowExperimentTracker:\n    def __init__(self, tracking_uri: str = \"http://localhost:5000\"):\n        mlflow.set_tracking_uri(tracking_uri)\n        self.experiment_id = None\n        self.run_id = None\n\n    def get_or_create_experiment(self, exp_name: str) -> str:\n        \"\"\"Get or create experiment by name\"\"\"\n        try:\n            exp = mlflow.get_experiment_by_name(exp_name)\n            if exp:\n                self.experiment_id = exp.experiment_id\n                return exp.experiment_id\n            \n            exp_id = mlflow.create_experiment(exp_name)\n            self.experiment_id = exp_id\n            return exp_id\n        except Exception as e:\n            print(f\"[MLflow] Failed to get/create experiment: {e}\", file=sys.stderr)\n            raise\n\n    def start_run(self, exp_id: str, run_name: Optional[str] = None) -> str:\n        \"\"\"Start a new MLflow run\"\"\"\n        try:\n            with mlflow.start_run(experiment_id=exp_id, run_name=run_name) as run:\n                self.run_id = run.info.run_id\n                return run.info.run_id\n        except Exception as e:\n            print(f\"[MLflow] Failed to start run: {e}\", file=sys.stderr)\n            raise\n\n    def log_metrics(self, metrics: Dict[str, float], step: int = 0) -> None:\n        \"\"\"Log metrics to current run\"\"\"\n        try:\n            for key, value in metrics.items():\n                mlflow.log_metric(key, value, step=step)\n        except Exception as e:\n            print(f\"[MLflow] Failed to log metrics: {e}\", file=sys.stderr)\n            raise\n\n    def log_params(self, params: Dict[str, Any]) -> None:\n        \"\"\"Log parameters to current run\"\"\"\n        try:\n            for key, value in params.items():\n                mlflow.log_param(key, str(value))\n        except Exception as e:\n            print(f\"[MLflow] Failed to log params: {e}\", file=sys.stderr)\n            raise\n\n    def log_model(self, model: Any, artifact_path: str, model_type: str = \"sklearn\") -> None:\n        \"\"\"Log model artifact\"\"\"\n        try:\n            if model_type == \"sklearn\":\n                mlflow.sklearn.log_model(model, artifact_path)\n            else:\n                mlflow.log_artifact(artifact_path)\n        except Exception as e:\n            print(f\"[MLflow] Failed to log model: {e}\", file=sys.stderr)\n            raise\n\n    def end_run(self, status: str = \"FINISHED\") -> None:\n        \"\"\"End current run\"\"\"\n        try:\n            mlflow.end_run(status=status)\n            self.run_id = None\n        except Exception as e:\n            print(f\"[MLflow] Failed to end run: {e}\", file=sys.stderr)\n            raise\n\n    def register_model(self, model_uri: str, model_name: str) -> str:\n        \"\"\"Register model to registry\"\"\"\n        try:\n            result = mlflow.register_model(model_uri, model_name)\n            return result.version\n        except Exception as e:\n            print(f\"[MLflow] Failed to register model: {e}\", file=sys.stderr)\n            raise\n\n    def transition_model_stage(self, model_name: str, version: str, stage: str) -> None:\n        \"\"\"Transition model to new stage\"\"\"\n        try:\n            client = mlflow.tracking.MlflowClient()\n            client.transition_model_version_stage(model_name, version, stage)\n        except Exception as e:\n            print(f\"[MLflow] Failed to transition model: {e}\", file=sys.stderr)\n            raise\n\n    def get_best_run(self, exp_id: str, metric_name: str = \"ndcg@3\") -> Dict[str, Any]:\n        \"\"\"Get best run for experiment\"\"\"\n        try:\n            client = mlflow.tracking.MlflowClient()\n            runs = client.search_runs(\n                experiment_ids=[exp_id],\n                order_by=[f\"metrics.{metric_name} DESC\"],\n                max_results=1\n            )\n            if runs:\n                return {\n                    \"run_id\": runs[0].info.run_id,\n                    \"metrics\": dict(runs[0].data.metrics),\n                    \"params\": dict(runs[0].data.params),\n                }\n            return None\n        except Exception as e:\n            print(f\"[MLflow] Failed to get best run: {e}\", file=sys.stderr)\n            raise\n\ndef main():\n    \"\"\"CLI interface for MLflow operations\"\"\"\n    if len(sys.argv) < 2:\n        print(\"Usage: mlflow_service.py <command> [args]\", file=sys.stderr)\n        sys.exit(1)\n    \n    command = sys.argv[1]\n    tracker = MLflowExperimentTracker()\n    \n    try:\n        if command == \"create_experiment\":\n            exp_name = sys.argv[2]\n            exp_id = tracker.get_or_create_experiment(exp_name)\n            print(json.dumps({\"success\": True, \"experiment_id\": exp_id}))\n        \n        elif command == \"start_run\":\n            exp_id = sys.argv[2]\n            run_name = sys.argv[3] if len(sys.argv) > 3 else None\n            run_id = tracker.start_run(exp_id, run_name)\n            print(json.dumps({\"success\": True, \"run_id\": run_id}))\n        \n        elif command == \"log_metrics\":\n            metrics_json = sys.argv[2]\n            metrics = json.loads(metrics_json)\n            tracker.log_metrics(metrics)\n            print(json.dumps({\"success\": True}))\n        \n        elif command == \"log_params\":\n            params_json = sys.argv[2]\n            params = json.loads(params_json)\n            tracker.log_params(params)\n            print(json.dumps({\"success\": True}))\n        \n        elif command == \"end_run\":\n            status = sys.argv[2] if len(sys.argv) > 2 else \"FINISHED\"\n            tracker.end_run(status)\n            print(json.dumps({\"success\": True}))\n        \n        elif command == \"get_best_run\":\n            exp_id = sys.argv[2]\n            metric_name = sys.argv[3] if len(sys.argv) > 3 else \"ndcg@3\"\n            result = tracker.get_best_run(exp_id, metric_name)\n            print(json.dumps({\"success\": True, \"run\": result}))\n        \n        else:\n            print(json.dumps({\"success\": False, \"error\": f\"Unknown command: {command}\"}))\n            sys.exit(1)\n    \n    except Exception as e:\n        print(json.dumps({\"success\": False, \"error\": str(e)}))\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n
