#!/usr/bin/env python3\nimport json\nimport sys\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Tuple\nimport numpy as np\nfrom scipy import stats\n\nclass DriftDetector:\n    \"\"\"Detects data, prediction, and concept drift\"\"\"\n    \n    def __init__(self, threshold: float = 0.05):\n        self.threshold = threshold\n        self.baseline_features = None\n        self.baseline_predictions = None\n        self.baseline_outcomes = None\n    \n    def set_baseline(self, features: List[float], predictions: List[float], outcomes: List[float]):\n        \"\"\"Set baseline distributions for drift detection\"\"\"\n        self.baseline_features = np.array(features)\n        self.baseline_predictions = np.array(predictions)\n        self.baseline_outcomes = np.array(outcomes)\n    \n    def detect_data_drift(self, current_features: List[float]) -> Tuple[bool, float]:\n        \"\"\"Detect data drift using Kolmogorov-Smirnov test\"\"\"\n        if self.baseline_features is None:\n            return False, 0.0\n        \n        current = np.array(current_features)\n        statistic, p_value = stats.ks_2samp(self.baseline_features, current)\n        \n        is_drift = p_value < self.threshold\n        return is_drift, float(p_value)\n    \n    def detect_prediction_drift(self, current_predictions: List[float]) -> Tuple[bool, float]:\n        \"\"\"Detect prediction drift using Chi-square test\"\"\"\n        if self.baseline_predictions is None:\n            return False, 0.0\n        \n        current = np.array(current_predictions)\n        statistic, p_value = stats.ks_2samp(self.baseline_predictions, current)\n        \n        is_drift = p_value < self.threshold\n        return is_drift, float(p_value)\n    \n    def detect_concept_drift(self, current_outcomes: List[float], performance_threshold: float = 0.02) -> Tuple[bool, float]:\n        \"\"\"Detect concept drift by comparing outcome distributions\"\"\"\n        if self.baseline_outcomes is None:\n            return False, 0.0\n        \n        current = np.array(current_outcomes)\n        baseline_mean = np.mean(self.baseline_outcomes)\n        current_mean = np.mean(current)\n        \n        drift_magnitude = abs(baseline_mean - current_mean) / (baseline_mean + 1e-10)\n        is_drift = drift_magnitude > performance_threshold\n        \n        return is_drift, float(drift_magnitude)\n\nclass RetrainingOrchestrator:\n    \"\"\"Orchestrates automated model retraining based on drift detection\"\"\"\n    \n    def __init__(self):\n        self.drift_detector = DriftDetector(threshold=0.05)\n        self.last_retrain_time = None\n        self.retrain_cooldown = timedelta(hours=1)\n        self.retrain_history = []\n    \n    def should_retrain(self, \n                      current_features: List[float],\n                      current_predictions: List[float],\n                      current_outcomes: List[float],\n                      performance_drop: float = 0.02) -> Dict[str, any]:\n        \"\"\"Determine if model should be retrained\"\"\"\n        \n        now = datetime.now()\n        \n        if self.last_retrain_time and (now - self.last_retrain_time) < self.retrain_cooldown:\n            return {\n                \"should_retrain\": False,\n                \"reason\": \"Cooldown period active\",\n                \"next_eligible_time\": (self.last_retrain_time + self.retrain_cooldown).isoformat()\n            }\n        \n        data_drift, data_drift_pvalue = self.drift_detector.detect_data_drift(current_features)\n        pred_drift, pred_drift_pvalue = self.drift_detector.detect_prediction_drift(current_predictions)\n        concept_drift, concept_drift_magnitude = self.drift_detector.detect_concept_drift(\n            current_outcomes, \n            performance_threshold=performance_drop\n        )\n        \n        triggers = []\n        if data_drift:\n            triggers.append(f\"Data drift detected (p={data_drift_pvalue:.4f})\")\n        if pred_drift:\n            triggers.append(f\"Prediction drift detected (p={pred_drift_pvalue:.4f})\")\n        if concept_drift:\n            triggers.append(f\"Concept drift detected (magnitude={concept_drift_magnitude:.4f})\")\n        \n        should_retrain = len(triggers) > 0\n        \n        if should_retrain:\n            self.last_retrain_time = now\n            self.retrain_history.append({\n                \"timestamp\": now.isoformat(),\n                \"triggers\": triggers,\n                \"data_drift\": data_drift,\n                \"pred_drift\": pred_drift,\n                \"concept_drift\": concept_drift\n            })\n        \n        return {\n            \"should_retrain\": should_retrain,\n            \"triggers\": triggers,\n            \"data_drift\": {\n                \"detected\": data_drift,\n                \"p_value\": float(data_drift_pvalue)\n            },\n            \"prediction_drift\": {\n                \"detected\": pred_drift,\n                \"p_value\": float(pred_drift_pvalue)\n            },\n            \"concept_drift\": {\n                \"detected\": concept_drift,\n                \"magnitude\": float(concept_drift_magnitude)\n            },\n            \"timestamp\": now.isoformat()\n        }\n    \n    def get_retrain_history(self) -> List[Dict]:\n        \"\"\"Get retraining history\"\"\"\n        return self.retrain_history\n\nclass ABTestingFramework:\n    \"\"\"A/B testing framework for ensemble weight optimization\"\"\"\n    \n    def __init__(self):\n        self.active_tests = {}\n        self.test_results = {}\n    \n    def create_test(self, test_id: str, control_weights: Dict[str, float], \n                   treatment_weights: Dict[str, float], \n                   traffic_split: float = 0.1,\n                   min_duration_hours: int = 24) -> Dict:\n        \"\"\"Create new A/B test\"\"\"\n        \n        test_config = {\n            \"test_id\": test_id,\n            \"created_at\": datetime.now().isoformat(),\n            \"control_weights\": control_weights,\n            \"treatment_weights\": treatment_weights,\n            \"traffic_split\": traffic_split,\n            \"min_duration_hours\": min_duration_hours,\n            \"status\": \"active\",\n            \"control_metrics\": {\"predictions\": 0, \"wins\": 0, \"ndcg\": 0},\n            \"treatment_metrics\": {\"predictions\": 0, \"wins\": 0, \"ndcg\": 0}\n        }\n        \n        self.active_tests[test_id] = test_config\n        return test_config\n    \n    def record_prediction(self, test_id: str, variant: str, \n                         ndcg_score: float, is_win: bool) -> None:\n        \"\"\"Record prediction result for A/B test\"\"\"\n        \n        if test_id not in self.active_tests:\n            raise ValueError(f\"Test {test_id} not found\")\n        \n        test = self.active_tests[test_id]\n        metrics_key = f\"{variant}_metrics\"\n        \n        if metrics_key not in test:\n            raise ValueError(f\"Invalid variant: {variant}\")\n        \n        test[metrics_key][\"predictions\"] += 1\n        if is_win:\n            test[metrics_key][\"wins\"] += 1\n        test[metrics_key][\"ndcg\"] += ndcg_score\n    \n    def get_test_results(self, test_id: str) -> Dict:\n        \"\"\"Get current test results\"\"\"\n        \n        if test_id not in self.active_tests:\n            raise ValueError(f\"Test {test_id} not found\")\n        \n        test = self.active_tests[test_id]\n        \n        control_ndcg = test[\"control_metrics\"][\"ndcg\"] / max(test[\"control_metrics\"][\"predictions\"], 1)\n        treatment_ndcg = test[\"treatment_metrics\"][\"ndcg\"] / max(test[\"treatment_metrics\"][\"predictions\"], 1)\n        \n        improvement = ((treatment_ndcg - control_ndcg) / (control_ndcg + 1e-10)) * 100\n        \n        return {\n            \"test_id\": test_id,\n            \"status\": test[\"status\"],\n            \"control\": {\n                \"predictions\": test[\"control_metrics\"][\"predictions\"],\n                \"wins\": test[\"control_metrics\"][\"wins\"],\n                \"avg_ndcg\": float(control_ndcg),\n                \"win_rate\": test[\"control_metrics\"][\"wins\"] / max(test[\"control_metrics\"][\"predictions\"], 1)\n            },\n            \"treatment\": {\n                \"predictions\": test[\"treatment_metrics\"][\"predictions\"],\n                \"wins\": test[\"treatment_metrics\"][\"wins\"],\n                \"avg_ndcg\": float(treatment_ndcg),\n                \"win_rate\": test[\"treatment_metrics\"][\"wins\"] / max(test[\"treatment_metrics\"][\"predictions\"], 1)\n            },\n            \"improvement_percent\": float(improvement),\n            \"is_significant\": abs(improvement) > 1.0\n        }\n    \n    def conclude_test(self, test_id: str) -> Dict:\n        \"\"\"Conclude A/B test and return winner\"\"\"\n        \n        if test_id not in self.active_tests:\n            raise ValueError(f\"Test {test_id} not found\")\n        \n        results = self.get_test_results(test_id)\n        test = self.active_tests[test_id]\n        \n        winner = \"treatment\" if results[\"improvement_percent\"] > 0 else \"control\"\n        winner_weights = test[\"treatment_weights\"] if winner == \"treatment\" else test[\"control_weights\"]\n        \n        test[\"status\"] = \"concluded\"\n        test[\"winner\"] = winner\n        test[\"winner_weights\"] = winner_weights\n        test[\"concluded_at\"] = datetime.now().isoformat()\n        \n        self.test_results[test_id] = results\n        \n        return {\n            \"test_id\": test_id,\n            \"winner\": winner,\n            \"improvement_percent\": results[\"improvement_percent\"],\n            \"winner_weights\": winner_weights,\n            \"results\": results\n        }\n\ndef main():\n    \"\"\"CLI interface for retraining orchestration\"\"\"\n    \n    if len(sys.argv) < 2:\n        print(\"Usage: autoRetrainingOrchestrator.py <command> [args]\", file=sys.stderr)\n        sys.exit(1)\n    \n    command = sys.argv[1]\n    \n    try:\n        if command == \"check_drift\":\n            features_json = sys.argv[2]\n            predictions_json = sys.argv[3]\n            outcomes_json = sys.argv[4]\n            \n            features = json.loads(features_json)\n            predictions = json.loads(predictions_json)\n            outcomes = json.loads(outcomes_json)\n            \n            orchestrator = RetrainingOrchestrator()\n            result = orchestrator.should_retrain(features, predictions, outcomes)\n            print(json.dumps(result))\n        \n        elif command == \"create_ab_test\":\n            test_id = sys.argv[2]\n            control_json = sys.argv[3]\n            treatment_json = sys.argv[4]\n            \n            control_weights = json.loads(control_json)\n            treatment_weights = json.loads(treatment_json)\n            \n            framework = ABTestingFramework()\n            result = framework.create_test(test_id, control_weights, treatment_weights)\n            print(json.dumps(result))\n        \n        else:\n            print(json.dumps({\"error\": f\"Unknown command: {command}\"}))\n            sys.exit(1)\n    \n    except Exception as e:\n        print(json.dumps({\"error\": str(e)}))\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n
